{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a2de7b",
   "metadata": {},
   "source": [
    "### Installing dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbcaec8",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef58db",
   "metadata": {},
   "source": [
    "### Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce800a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dataset = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(example):\n",
    "    ids = encode_dataset.encode_ordinary(example[\"text\"])\n",
    "    out = {\"ids\": ids, \"len\":len(ids)}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = dataset.map(\n",
    "        process,\n",
    "        remove_columns=[\"text\"],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=8\n",
    "    )\n",
    "\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset[\"len\"], dtype=np.uint64)\n",
    "        file_name = f\"{split}.bin\"\n",
    "        data_type = np.uint16\n",
    "        arr = np.memmap(file_name, dtype=data_type, mode=\"w+\", shape=(arr_len,))\n",
    "        total_batches = 1024\n",
    "\n",
    "        idx = 0\n",
    "\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f\"writing {file_name}\"):\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format(\"numpy\")\n",
    "            arr_batch = np.concatenate(batch[\"ids\"])\n",
    "\n",
    "            arr[idx:idx + len(arr_batch)] = arr_batch\n",
    "\n",
    "            idx += len(arr_batch)\n",
    "\n",
    "        arr.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d1af0",
   "metadata": {},
   "source": [
    "# Create input-output batches for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f6bf6",
   "metadata": {},
   "source": [
    "### Define SLM Training Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "max_iters = 20000\n",
    "warmup_steps = 1000\n",
    "min_lr = 5e-4\n",
    "eval_iters = 500\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "gradient_accumulation_steps = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"\n",
    "\n",
    "data_type = \"bfloat16\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"float16\"\n",
    "\n",
    "ptdtype = {\"float32\": torch.float32, \"bfloat16\":torch.bfloat16, \"float16\":torch.float16}[data_type]\n",
    "\n",
    "ctx = nullcontext() if device_type == \"cpu\" else torch.amp.autocast_mode(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device=device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7480101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "\n",
    "    if split == \"train\":\n",
    "        data = np.memmap(\"train.bin\", dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "    else:\n",
    "        data = np.memmap(\"validation.bin\", dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1 : i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "\n",
    "    if device_type == \"cuda\":\n",
    "\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d7e0a3",
   "metadata": {},
   "source": [
    "### Define the SLM model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "    \n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, \"scaled_dot_product_attention\")\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                 .view(1, 1, config.block_size, config.block_size))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_head, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "# Transformer block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_mebd, config.volab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "        else:\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        \n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da4a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=50257,\n",
    "    block_size=128,\n",
    "    n_layer=6, \n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bais=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea013c54",
   "metadata": {},
   "source": [
    "### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17fa0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc2e51",
   "metadata": {},
   "source": [
    "### Define SLM Training Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9)\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer=optimizer, total_iters=warmup_steps)\n",
    "scheduler_decay = CosineAnnealingLR(optimizer=optimizer, T_max=max_iters - warmup_steps, eta_min=min_lr)\n",
    "scheduler = SequentialLR(optimizer=optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(data_type == \"float16\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71df16",
   "metadata": {},
   "source": [
    "### Pre-train the SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4804c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "model = model.to(device=device)\n",
    "\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses[\"train\"]:.4f}, val_loss {losses[\"val\"]:.4f}\")\n",
    "        print(f\"\"\"The current learning rate: {optimizer.param_groups[0][\"lr\"]:.5f}\"\"\")\n",
    "        train_loss_list += [losses[\"train\"]]\n",
    "        validation_loss_list += [losses[\"val\"]]\n",
    "\n",
    "        if losses[\"val\"] < best_val_loss:\n",
    "            best_val_loss = losses[\"val\"]\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    x, y = get_batch(\"train\")\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer=optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4b20a",
   "metadata": {},
   "source": [
    "### Plot the SLM loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d123b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
    "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list_converted, \"g\", label=\"train_loss\")\n",
    "plt.plot(validation_loss_list_converted, \"r\", label=\"validation_loss\")\n",
    "plt.xlabel(\"Steps - Every 100 epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79514937",
   "metadata": {},
   "source": [
    "### Run SLM inference on our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8304733",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(config)\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Once upon a time there was a pumpkin\"\n",
    "\n",
    "context = (torch.tensor(encode_dataset.encode_ordinary(sentence)).unsqueeze(dim=0))\n",
    "y = model.generate(context, 200)\n",
    "print(encode_dataset.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"A little girl went to the woods\"\n",
    "\n",
    "context = (torch.tensor(encode_dataset.encode_ordinary(sentence)).unsqueeze(dim=0))\n",
    "y = model.generate(context, 200)\n",
    "print(encode_dataset.decode(y.squeeze().tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
