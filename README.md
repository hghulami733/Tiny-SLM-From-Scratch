# Tiny-SLM-From-Scratch

This project implements a fully custom Small Language Model (SLM) trained on the TinyStories dataset using a transformer architecture written from scratch.
It is designed for clarity, educational value, and extendability toward more advanced SLM designs.

Features

Custom dataset loader (TinyStories)

Full transformer implementation

Multi-head attention

Feedforward blocks

Positional encodings

Causal LM head

Training loop (AdamW + weight decay)

tiktoken integration

Checkpoint saving & loading
